{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9249ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "from experiments.iter1.util.importer import *\n",
    "nist_path = \"../../src/data_preprocess/dataset/NIST_cleaned.csv\"\n",
    "uk_path = \"../../src/data_preprocess/dataset/UKDATA_cleaned.csv\"\n",
    "\n",
    "from experiments.iter1.util.evaluate import evaluate_model, plot_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47c4fd",
   "metadata": {},
   "source": [
    "# Nist\n",
    "\n",
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e4cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]NRMSE Loss: tensor(0.1875)\n",
      "NMAE Loss: tensor(0.0167)\n",
      "NMAXE Loss: tensor(0.7592)\n",
      "NRMSE Loss: tensor(0.1875)\n",
      "Testing DataLoader 0:   4%|▍         | 1/24 [00:01<00:30,  0.75it/s]NRMSE Loss: tensor(0.2444)\n",
      "NMAE Loss: tensor(0.0135)\n",
      "NMAXE Loss: tensor(1.3120)\n",
      "NRMSE Loss: tensor(0.2444)\n",
      "Testing DataLoader 0:   8%|▊         | 2/24 [00:02<00:30,  0.73it/s]NRMSE Loss: tensor(0.1853)\n",
      "NMAE Loss: tensor(0.0095)\n",
      "NMAXE Loss: tensor(0.6550)\n",
      "NRMSE Loss: tensor(0.1853)\n",
      "Testing DataLoader 0:  12%|█▎        | 3/24 [00:04<00:28,  0.73it/s]NRMSE Loss: tensor(0.1232)\n",
      "NMAE Loss: tensor(0.0053)\n",
      "NMAXE Loss: tensor(0.5549)\n",
      "NRMSE Loss: tensor(0.1232)\n",
      "Testing DataLoader 0:  17%|█▋        | 4/24 [00:04<00:24,  0.81it/s]NRMSE Loss: tensor(0.1180)\n",
      "NMAE Loss: tensor(0.0057)\n",
      "NMAXE Loss: tensor(0.6393)\n",
      "NRMSE Loss: tensor(0.1180)\n",
      "Testing DataLoader 0:  21%|██        | 5/24 [00:06<00:23,  0.81it/s]NRMSE Loss: tensor(0.1663)\n",
      "NMAE Loss: tensor(0.0053)\n",
      "NMAXE Loss: tensor(0.7830)\n",
      "NRMSE Loss: tensor(0.1663)\n",
      "Testing DataLoader 0:  25%|██▌       | 6/24 [00:07<00:22,  0.81it/s]NRMSE Loss: tensor(0.1591)\n",
      "NMAE Loss: tensor(0.0053)\n",
      "NMAXE Loss: tensor(0.7520)\n",
      "NRMSE Loss: tensor(0.1591)\n",
      "Testing DataLoader 0:  29%|██▉       | 7/24 [00:08<00:20,  0.81it/s]NRMSE Loss: tensor(0.0970)\n",
      "NMAE Loss: tensor(0.0030)\n",
      "NMAXE Loss: tensor(0.4464)\n",
      "NRMSE Loss: tensor(0.0970)\n",
      "Testing DataLoader 0:  33%|███▎      | 8/24 [00:09<00:19,  0.81it/s]NRMSE Loss: tensor(0.1177)\n",
      "NMAE Loss: tensor(0.0029)\n",
      "NMAXE Loss: tensor(0.5328)\n",
      "NRMSE Loss: tensor(0.1177)\n",
      "Testing DataLoader 0:  38%|███▊      | 9/24 [00:11<00:18,  0.81it/s]NRMSE Loss: tensor(0.1320)\n",
      "NMAE Loss: tensor(0.0036)\n",
      "NMAXE Loss: tensor(0.5014)\n",
      "NRMSE Loss: tensor(0.1320)\n",
      "Testing DataLoader 0:  42%|████▏     | 10/24 [00:12<00:17,  0.81it/s]NRMSE Loss: tensor(0.1445)\n",
      "NMAE Loss: tensor(0.0042)\n",
      "NMAXE Loss: tensor(0.5805)\n",
      "NRMSE Loss: tensor(0.1445)\n",
      "Testing DataLoader 0:  46%|████▌     | 11/24 [00:13<00:16,  0.81it/s]NRMSE Loss: tensor(0.1490)\n",
      "NMAE Loss: tensor(0.0050)\n",
      "NMAXE Loss: tensor(0.6116)\n",
      "NRMSE Loss: tensor(0.1490)\n",
      "Testing DataLoader 0:  50%|█████     | 12/24 [00:14<00:14,  0.81it/s]NRMSE Loss: tensor(0.1313)\n",
      "NMAE Loss: tensor(0.0043)\n",
      "NMAXE Loss: tensor(0.3949)\n",
      "NRMSE Loss: tensor(0.1313)\n",
      "Testing DataLoader 0:  54%|█████▍    | 13/24 [00:16<00:13,  0.80it/s]NRMSE Loss: tensor(0.1784)\n",
      "NMAE Loss: tensor(0.0095)\n",
      "NMAXE Loss: tensor(0.9293)\n",
      "NRMSE Loss: tensor(0.1784)\n",
      "Testing DataLoader 0:  58%|█████▊    | 14/24 [00:17<00:12,  0.81it/s]NRMSE Loss: tensor(0.1922)\n",
      "NMAE Loss: tensor(0.0117)\n",
      "NMAXE Loss: tensor(0.5029)\n",
      "NRMSE Loss: tensor(0.1922)\n",
      "Testing DataLoader 0:  62%|██████▎   | 15/24 [00:18<00:11,  0.81it/s]NRMSE Loss: tensor(0.2114)\n",
      "NMAE Loss: tensor(0.0152)\n",
      "NMAXE Loss: tensor(0.7765)\n",
      "NRMSE Loss: tensor(0.2114)\n",
      "Testing DataLoader 0:  67%|██████▋   | 16/24 [00:19<00:09,  0.81it/s]NRMSE Loss: tensor(0.1064)\n",
      "NMAE Loss: tensor(0.0058)\n",
      "NMAXE Loss: tensor(0.6116)\n",
      "NRMSE Loss: tensor(0.1064)\n",
      "Testing DataLoader 0:  71%|███████   | 17/24 [00:21<00:08,  0.79it/s]NRMSE Loss: tensor(0.1463)\n",
      "NMAE Loss: tensor(0.0049)\n",
      "NMAXE Loss: tensor(0.5549)\n",
      "NRMSE Loss: tensor(0.1463)\n",
      "Testing DataLoader 0:  75%|███████▌  | 18/24 [00:22<00:07,  0.79it/s]NRMSE Loss: tensor(0.1561)\n",
      "NMAE Loss: tensor(0.0057)\n",
      "NMAXE Loss: tensor(0.6414)\n",
      "NRMSE Loss: tensor(0.1561)\n",
      "Testing DataLoader 0:  79%|███████▉  | 19/24 [00:23<00:06,  0.81it/s]NRMSE Loss: tensor(0.1180)\n",
      "NMAE Loss: tensor(0.0054)\n",
      "NMAXE Loss: tensor(0.3719)\n",
      "NRMSE Loss: tensor(0.1180)\n",
      "Testing DataLoader 0:  83%|████████▎ | 20/24 [00:24<00:04,  0.82it/s]NRMSE Loss: tensor(0.1331)\n",
      "NMAE Loss: tensor(0.0052)\n",
      "NMAXE Loss: tensor(0.4592)\n",
      "NRMSE Loss: tensor(0.1331)\n",
      "Testing DataLoader 0:  88%|████████▊ | 21/24 [00:24<00:03,  0.84it/s]NRMSE Loss: tensor(0.1226)\n",
      "NMAE Loss: tensor(0.0049)\n",
      "NMAXE Loss: tensor(0.4492)\n",
      "NRMSE Loss: tensor(0.1226)\n",
      "Testing DataLoader 0:  92%|█████████▏| 22/24 [00:25<00:02,  0.86it/s]NRMSE Loss: tensor(0.1266)\n",
      "NMAE Loss: tensor(0.0047)\n",
      "NMAXE Loss: tensor(0.4633)\n",
      "NRMSE Loss: tensor(0.1266)\n",
      "Testing DataLoader 0:  96%|█████████▌| 23/24 [00:26<00:01,  0.87it/s]NRMSE Loss: tensor(0.1477)\n",
      "NMAE Loss: tensor(0.0056)\n",
      "NMAXE Loss: tensor(0.5589)\n",
      "NRMSE Loss: tensor(0.1477)\n",
      "Testing DataLoader 0: 100%|██████████| 24/24 [00:26<00:00,  0.90it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "    NMAE Loss: _epoch      0.0068085468374192715\n",
      "   NMAXE Loss: _epoch       0.6194103956222534\n",
      "     NMCRPS: _epoch         0.34970057010650635\n",
      "   NRMSE Loss: _epoch       0.14979377388954163\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Calculating On set...\n",
      "DATA LEN:  384\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m nist_lstm_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(MODEL_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     48\u001b[0m nist_lstm_test_results \u001b[38;5;241m=\u001b[39m nist_lstm_model\u001b[38;5;241m.\u001b[39mtest()\n\u001b[0;32m---> 50\u001b[0m nist_lstm_flex_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnist_lstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleaner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTIMESTAMP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPOWER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_limit_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moff_limit_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsecutive_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_horizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTARGET_COLUMN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_boundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m nist_lstm_results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnist_lstm_test_results, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnist_lstm_flex_results}\n\u001b[1;32m     53\u001b[0m nist_lstm_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/P5/experimentation/experiments/iter1/util/evaluate.py:56\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, df, splitter, cleaner, TIMESTAMP, POWER, on_limit_w, off_limit_w, consecutive_points, seq_len, time_horizon, TARGET_COLUMN, error, temp_boundary, confidence)\u001b[0m\n\u001b[1;32m     53\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator(model, error, temp_boundary)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating On set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mon_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_horizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTARGET_COLUMN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     57\u001b[0m on_mafe \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(mafe)\n\u001b[1;32m     58\u001b[0m on_maofe \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(maofe)\n",
      "File \u001b[0;32m~/P5/experimentation/src/util/evaluator.py:46\u001b[0m, in \u001b[0;36mEvaluator.init_predictions\u001b[0;34m(self, data, seq_len, time_horizon, target_column, confidence)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result_predictions, \u001b[38;5;28mlist\u001b[39m): result_predictions \u001b[38;5;241m=\u001b[39m result_predictions\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result_predictions, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     actual_flex \u001b[38;5;241m=\u001b[39m \u001b[43mflex_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_actual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_boundery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_boundery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     predicted_flex, probabilities \u001b[38;5;241m=\u001b[39m prob_flex_predict(result_predictions, lower_boundery, upper_boundery, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror, confidence\u001b[38;5;241m=\u001b[39mconfidence)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflex_actual_values\u001b[38;5;241m.\u001b[39mappend(actual_flex)\n",
      "File \u001b[0;32m~/P5/experimentation/src/util/flex_predict.py:6\u001b[0m, in \u001b[0;36mflex_predict\u001b[0;34m(forecasts, lower_bound, upper_bound, error)\u001b[0m\n\u001b[1;32m      4\u001b[0m flexibility \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m forecast \u001b[38;5;129;01min\u001b[39;00m forecasts:\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lower_bound \u001b[38;5;241m+\u001b[39m error \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m forecast \u001b[38;5;129;01mand\u001b[39;00m upper_bound \u001b[38;5;241m-\u001b[39m error \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m forecast:    \n\u001b[1;32m      7\u001b[0m         flexibility \u001b[38;5;241m=\u001b[39m flexibility \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "# Model\n",
    "input_size = 4\n",
    "time_horizon = 4\n",
    "hidden_size = 64\n",
    "num_epochs = 100\n",
    "seq_len = 152\n",
    "num_layers = 1\n",
    "MODEL_PATH = 'model_saves/ex1_model'\n",
    "\n",
    "\n",
    "df = pd.read_csv(nist_path)\n",
    "\n",
    "cleaner = TempCleaner(clean_pow_low, clean_in_low, clean_in_high, clean_out_low, clean_out_high, clean_delta_temp)\n",
    "splitter = DaySplitter(TIMESTAMP, POWER ,train_days, val_days, test_days)\n",
    "    \n",
    "nist_lstm_model = LSTM(hidden_size, num_layers, input_size, time_horizon, dropout)\n",
    "trainer = TrainerWrapper(L.Trainer, \n",
    "                         max_epochs=num_epochs, \n",
    "                         callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)], \n",
    "                         gradient_clip_val=gradient_clipping)\n",
    "optimizer = OptimizerWrapper(optim.Adam, nist_lstm_model, lr=learning_rate)\n",
    "\n",
    "nist_lstm_model = MonteCarloPipeline.Builder() \\\n",
    "    .add_data(df) \\\n",
    "    .set_cleaner(cleaner) \\\n",
    "    .set_normalizer_class(MinMaxNormalizer) \\\n",
    "    .set_splitter(splitter) \\\n",
    "    .set_sequencer_class(AllTimeSequencer) \\\n",
    "    .set_target_column(TARGET_COLUMN) \\\n",
    "    .set_model(nist_lstm_model) \\\n",
    "    .set_optimizer(optimizer) \\\n",
    "    .set_batch_size(batch_size) \\\n",
    "    .set_seq_len(seq_len) \\\n",
    "    .set_worker_num(NUM_WORKERS) \\\n",
    "    .set_error(NRMSE) \\\n",
    "    .set_train_error(RMSE) \\\n",
    "    .add_test_error(NMAE) \\\n",
    "    .add_test_error(NMAXE) \\\n",
    "    .set_trainer(trainer) \\\n",
    "    .set_inference_samples(inference_samples) \\\n",
    "    .set_inference_dropout(inference_dropout) \\\n",
    "    .add_test_error(NRMSE) \\\n",
    "    .add_test_error(NMCRPS) \\\n",
    "    .build()\n",
    "\n",
    "nist_lstm_model.load_state_dict(torch.load(MODEL_PATH + \".pth\", weights_only=True))\n",
    "nist_lstm_test_results = nist_lstm_model.test()\n",
    "\n",
    "nist_lstm_flex_results = evaluate_model(nist_lstm_model, df, splitter, cleaner, TIMESTAMP, POWER, on_limit_w, off_limit_w, consecutive_points, seq_len, time_horizon, TARGET_COLUMN, error, temp_boundary, None)\n",
    "\n",
    "nist_lstm_results = {**nist_lstm_test_results, **nist_lstm_flex_results}\n",
    "nist_lstm_results[\"title\"] = \"LSTM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0b52e",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "# Model\n",
    "input_size = 4\n",
    "time_horizon = 4\n",
    "hidden_size = 128\n",
    "num_epochs = 100\n",
    "seq_len = 224\n",
    "num_layers = 1\n",
    "MODEL_PATH = 'model_saves/ex3_model'\n",
    "\n",
    "df = pd.read_csv(nist_path)\n",
    "\n",
    "cleaner = TempCleaner(clean_pow_low, clean_in_low, clean_in_high, clean_out_low, clean_out_high, clean_delta_temp)\n",
    "splitter = DaySplitter(TIMESTAMP, POWER ,train_days, val_days, test_days)\n",
    "    \n",
    "nist_GRU_model = GRU(hidden_size, num_layers, input_size, time_horizon, dropout)\n",
    "trainer = TrainerWrapper(L.Trainer, \n",
    "                         max_epochs=num_epochs, \n",
    "                         callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)], \n",
    "                         gradient_clip_val=gradient_clipping)\n",
    "optimizer = OptimizerWrapper(optim.Adam, nist_GRU_model, lr=learning_rate)\n",
    "\n",
    "nist_GRU_model = MonteCarloPipeline.Builder() \\\n",
    "    .add_data(df) \\\n",
    "    .set_cleaner(cleaner) \\\n",
    "    .set_normalizer_class(MinMaxNormalizer) \\\n",
    "    .set_splitter(splitter) \\\n",
    "    .set_sequencer_class(AllTimeSequencer) \\\n",
    "    .set_target_column(TARGET_COLUMN) \\\n",
    "    .set_model(nist_GRU_model) \\\n",
    "    .set_optimizer(optimizer) \\\n",
    "    .set_batch_size(batch_size) \\\n",
    "    .set_seq_len(seq_len) \\\n",
    "    .set_worker_num(NUM_WORKERS) \\\n",
    "    .set_error(NRMSE) \\\n",
    "    .set_train_error(RMSE) \\\n",
    "    .add_test_error(NMAE) \\\n",
    "    .add_test_error(NMAXE) \\\n",
    "    .set_trainer(trainer) \\\n",
    "    .set_inference_samples(inference_samples) \\\n",
    "    .set_inference_dropout(inference_dropout) \\\n",
    "    .add_test_error(NRMSE) \\\n",
    "    .add_test_error(NMCRPS) \\\n",
    "    .build()\n",
    "\n",
    "nist_GRU_model.load_state_dict(torch.load(MODEL_PATH + \".pth\", weights_only=True))\n",
    "nist_GRU_test_results = nist_GRU_model.test()\n",
    "\n",
    "nist_GRU_flex_results = evaluate_model(nist_GRU_model, df, splitter, cleaner, TIMESTAMP, POWER, on_limit_w, off_limit_w, consecutive_points, seq_len, time_horizon, TARGET_COLUMN, error, temp_boundary, None)\n",
    "\n",
    "nist_GRU_results = {**nist_GRU_test_results, **nist_GRU_flex_results}\n",
    "nist_GRU_results[\"title\"] = \"GRU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9886710",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0670c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper parameters\n",
    "# Model\n",
    "input_size = 4\n",
    "time_horizon = 4\n",
    "hidden_size = 48\n",
    "num_epochs = 100\n",
    "seq_len = 168\n",
    "num_layers = 1\n",
    "MODEL_PATH = 'model_saves/ex5_model'\n",
    "\n",
    "df = pd.read_csv(nist_path)\n",
    "\n",
    "cleaner = TempCleaner(clean_pow_low, clean_in_low, clean_in_high, clean_out_low, clean_out_high, clean_delta_temp)\n",
    "splitter = DaySplitter(TIMESTAMP, POWER ,train_days, val_days, test_days)\n",
    "    \n",
    "nist_TCN_model = TCN(hidden_size, num_layers, input_size, time_horizon, dropout, seq_len)\n",
    "trainer = TrainerWrapper(L.Trainer, \n",
    "                         max_epochs=num_epochs, \n",
    "                         callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)], \n",
    "                         gradient_clip_val=gradient_clipping)\n",
    "optimizer = OptimizerWrapper(optim.Adam, nist_TCN_model, lr=learning_rate)\n",
    "\n",
    "nist_TCN_model = MonteCarloPipeline.Builder() \\\n",
    "    .add_data(df) \\\n",
    "    .set_cleaner(cleaner) \\\n",
    "    .set_normalizer_class(MinMaxNormalizer) \\\n",
    "    .set_splitter(splitter) \\\n",
    "    .set_sequencer_class(AllTimeSequencer) \\\n",
    "    .set_target_column(TARGET_COLUMN) \\\n",
    "    .set_model(nist_TCN_model) \\\n",
    "    .set_optimizer(optimizer) \\\n",
    "    .set_batch_size(batch_size) \\\n",
    "    .set_seq_len(seq_len) \\\n",
    "    .set_worker_num(NUM_WORKERS) \\\n",
    "    .set_error(NRMSE) \\\n",
    "    .set_train_error(RMSE) \\\n",
    "    .add_test_error(NMAE) \\\n",
    "    .add_test_error(NMAXE) \\\n",
    "    .set_trainer(trainer) \\\n",
    "    .set_inference_samples(inference_samples) \\\n",
    "    .set_inference_dropout(inference_dropout) \\\n",
    "    .add_test_error(NRMSE) \\\n",
    "    .add_test_error(NMCRPS) \\\n",
    "    .build()\n",
    "\n",
    "nist_TCN_model.load_state_dict(torch.load(MODEL_PATH + \".pth\", weights_only=True))\n",
    "nist_TCN_test_results = nist_TCN_model.test()\n",
    "\n",
    "nist_TCN_flex_results = evaluate_model(nist_TCN_model, df, splitter, cleaner, TIMESTAMP, POWER, on_limit_w, off_limit_w, consecutive_points, seq_len, time_horizon, TARGET_COLUMN, error, temp_boundary, None)\n",
    "\n",
    "nist_TCN_results = {**nist_TCN_test_results, **nist_TCN_flex_results}\n",
    "nist_TCN_results[\"title\"] = \"TCN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3bf9e",
   "metadata": {},
   "source": [
    "# Nist Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_pillar_diagrams([\"on mafe\", \"on maufe\", \"on maofe\", \"on epfr\", \"off mafe\", \"off maufe\", \"off maofe\", \"off epfr\"], \n",
    "                     [[nist_lstm_results, nist_GRU_results, nist_TCN_results]],\n",
    "                     [\"Nist dataset\"],\n",
    "                     y_max=1.6)\n",
    "\n",
    "plot_pillar_diagrams([NRMSE.get_key(), NMAXE.get_key(), NMAE.get_key()], \n",
    "                     [[nist_lstm_results, nist_GRU_results, nist_TCN_results]],\n",
    "                     [\"Nist dataset\"],\n",
    "                     y_max=1)\n",
    "\n",
    "\n",
    "lstm_predictions = nist_lstm_model.get_predictions()\n",
    "gru_predictions = nist_GRU_model.get_predictions()\n",
    "tcn_predictions = nist_TCN_model.get_predictions()\n",
    "\n",
    "plot_models(\n",
    "    [lstm_predictions, gru_predictions, tcn_predictions],\n",
    "    time_horizon,\n",
    "    nist_lstm_model.get_timestamps(),\n",
    "    nist_lstm_model.get_actuals(),\n",
    "    titels=[\"LSTM\", \"GRU\", \"TCN\"])\n",
    "\n",
    "plot_boxplots([lstm_predictions, gru_predictions, tcn_predictions, nist_lstm_model.get_actuals()],\n",
    "              titles=[\"LSTM\", \"GRU\", \"TCN\", \"Actuals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad4d0d",
   "metadata": {},
   "source": [
    "# UK\n",
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5751792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "# Model\n",
    "input_size = 4\n",
    "time_horizon = 4\n",
    "hidden_size = 64\n",
    "num_epochs = 100\n",
    "seq_len = 152\n",
    "num_layers = 1\n",
    "MODEL_PATH = 'model_saves/ex2_model'\n",
    "\n",
    "df = pd.read_csv(uk_path)\n",
    "\n",
    "cleaner = TempCleaner(clean_pow_low, clean_in_low, clean_in_high, clean_out_low, clean_out_high, clean_delta_temp)\n",
    "splitter = DaySplitter(TIMESTAMP, POWER ,train_days, val_days, test_days)\n",
    "    \n",
    "uk_lstm_model = LSTM(hidden_size, num_layers, input_size, time_horizon, dropout)\n",
    "trainer = TrainerWrapper(L.Trainer, \n",
    "                         max_epochs=num_epochs, \n",
    "                         callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)], \n",
    "                         gradient_clip_val=gradient_clipping)\n",
    "optimizer = OptimizerWrapper(optim.Adam, uk_lstm_model, lr=learning_rate)\n",
    "\n",
    "uk_lstm_model = MonteCarloPipeline.Builder() \\\n",
    "    .add_data(df) \\\n",
    "    .set_cleaner(cleaner) \\\n",
    "    .set_normalizer_class(MinMaxNormalizer) \\\n",
    "    .set_splitter(splitter) \\\n",
    "    .set_sequencer_class(AllTimeSequencer) \\\n",
    "    .set_target_column(TARGET_COLUMN) \\\n",
    "    .set_model(uk_lstm_model) \\\n",
    "    .set_optimizer(optimizer) \\\n",
    "    .set_batch_size(batch_size) \\\n",
    "    .set_seq_len(seq_len) \\\n",
    "    .set_worker_num(NUM_WORKERS) \\\n",
    "    .set_error(NRMSE) \\\n",
    "    .set_train_error(RMSE) \\\n",
    "    .add_test_error(NMAE) \\\n",
    "    .add_test_error(NMAXE) \\\n",
    "    .set_trainer(trainer) \\\n",
    "    .set_inference_samples(inference_samples) \\\n",
    "    .set_inference_dropout(inference_dropout) \\\n",
    "    .add_test_error(NRMSE) \\\n",
    "    .add_test_error(NMLSCV) \\\n",
    "    .add_test_error(NMCRPS) \\\n",
    "    .build()\n",
    "\n",
    "uk_lstm_model.load_state_dict(torch.load(MODEL_PATH + \".pth\", weights_only=True))\n",
    "uk_lstm_test_results = uk_lstm_model.test()\n",
    "\n",
    "uk_lstm_flex_results = evaluate_model(uk_lstm_model, df, splitter, cleaner, TIMESTAMP, POWER, on_limit_w, off_limit_w, consecutive_points, seq_len, time_horizon, TARGET_COLUMN, error, temp_boundary, None)\n",
    "\n",
    "uk_lstm_results = {**uk_lstm_test_results, **uk_lstm_flex_results}\n",
    "uk_lstm_results[\"title\"] = \"LSTM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f578b",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a66ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper parameters\n",
    "# Model\n",
    "input_size = 4\n",
    "time_horizon = 4\n",
    "hidden_size = 128\n",
    "num_epochs = 100\n",
    "seq_len = 224\n",
    "num_layers = 1\n",
    "MODEL_PATH = 'model_saves/ex4_model'\n",
    "\n",
    "df = pd.read_csv(uk_path)\n",
    "\n",
    "cleaner = TempCleaner(clean_pow_low, clean_in_low, clean_in_high, clean_out_low, clean_out_high, clean_delta_temp)\n",
    "splitter = DaySplitter(TIMESTAMP, POWER ,train_days, val_days, test_days)\n",
    "    \n",
    "uk_GRU_model = GRU(hidden_size, num_layers, input_size, time_horizon, dropout)\n",
    "trainer = TrainerWrapper(L.Trainer, \n",
    "                         max_epochs=num_epochs, \n",
    "                         callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)], \n",
    "                         gradient_clip_val=gradient_clipping)\n",
    "optimizer = OptimizerWrapper(optim.Adam, uk_GRU_model, lr=learning_rate)\n",
    "\n",
    "uk_GRU_model = MonteCarloPipeline.Builder() \\\n",
    "    .add_data(df) \\\n",
    "    .set_cleaner(cleaner) \\\n",
    "    .set_normalizer_class(MinMaxNormalizer) \\\n",
    "    .set_splitter(splitter) \\\n",
    "    .set_sequencer_class(AllTimeSequencer) \\\n",
    "    .set_target_column(TARGET_COLUMN) \\\n",
    "    .set_model(uk_GRU_model) \\\n",
    "    .set_optimizer(optimizer) \\\n",
    "    .set_batch_size(batch_size) \\\n",
    "    .set_seq_len(seq_len) \\\n",
    "    .set_worker_num(NUM_WORKERS) \\\n",
    "    .set_error(NRMSE) \\\n",
    "    .set_train_error(RMSE) \\\n",
    "    .add_test_error(NMAE) \\\n",
    "    .add_test_error(NMAXE) \\\n",
    "    .set_trainer(trainer) \\\n",
    "    .set_inference_samples(inference_samples) \\\n",
    "    .set_inference_dropout(inference_dropout) \\\n",
    "    .add_test_error(NRMSE) \\\n",
    "    .add_test_error(NMCRPS) \\\n",
    "    .build()\n",
    "\n",
    "uk_GRU_model.load_state_dict(torch.load(MODEL_PATH + \".pth\", weights_only=True))\n",
    "uk_GRU_test_results = uk_GRU_model.test()\n",
    "\n",
    "uk_GRU_flex_results = evaluate_model(uk_GRU_model, df, splitter, cleaner, TIMESTAMP, POWER, on_limit_w, off_limit_w, consecutive_points, seq_len, time_horizon, TARGET_COLUMN, error, temp_boundary, None)\n",
    "\n",
    "uk_GRU_results = {**uk_GRU_test_results, **uk_GRU_flex_results}\n",
    "uk_GRU_results[\"title\"] = \"GRU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a629ca",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "# Model\n",
    "input_size = 4\n",
    "time_horizon = 4\n",
    "hidden_size = 48\n",
    "num_epochs = 100\n",
    "seq_len = 168\n",
    "num_layers = 1\n",
    "MODEL_PATH = 'model_saves/ex6_model'\n",
    "\n",
    "df = pd.read_csv(uk_path)\n",
    "\n",
    "cleaner = TempCleaner(clean_pow_low, clean_in_low, clean_in_high, clean_out_low, clean_out_high, clean_delta_temp)\n",
    "splitter = DaySplitter(TIMESTAMP, POWER ,train_days, val_days, test_days)\n",
    "    \n",
    "uk_TCN_model = TCN(hidden_size, num_layers, input_size, time_horizon, dropout, seq_len)\n",
    "trainer = TrainerWrapper(L.Trainer, \n",
    "                         max_epochs=num_epochs, \n",
    "                         callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)], \n",
    "                         gradient_clip_val=gradient_clipping)\n",
    "optimizer = OptimizerWrapper(optim.Adam, uk_TCN_model, lr=learning_rate)\n",
    "\n",
    "uk_TCN_model = MonteCarloPipeline.Builder() \\\n",
    "    .add_data(df) \\\n",
    "    .set_cleaner(cleaner) \\\n",
    "    .set_normalizer_class(MinMaxNormalizer) \\\n",
    "    .set_splitter(splitter) \\\n",
    "    .set_sequencer_class(AllTimeSequencer) \\\n",
    "    .set_target_column(TARGET_COLUMN) \\\n",
    "    .set_model(uk_TCN_model) \\\n",
    "    .set_optimizer(optimizer) \\\n",
    "    .set_batch_size(batch_size) \\\n",
    "    .set_seq_len(seq_len) \\\n",
    "    .set_worker_num(NUM_WORKERS) \\\n",
    "    .set_error(NRMSE) \\\n",
    "    .set_train_error(RMSE) \\\n",
    "    .add_test_error(NMAE) \\\n",
    "    .add_test_error(NMAXE) \\\n",
    "    .set_trainer(trainer) \\\n",
    "    .set_inference_samples(inference_samples) \\\n",
    "    .set_inference_dropout(inference_dropout) \\\n",
    "    .add_test_error(NRMSE) \\\n",
    "    .add_test_error(NMCRPS) \\\n",
    "    .build()\n",
    "\n",
    "uk_TCN_model.load_state_dict(torch.load(MODEL_PATH + \".pth\", weights_only=True))\n",
    "uk_TCN_test_results = uk_TCN_model.test()\n",
    "\n",
    "uk_TCN_flex_results = evaluate_model(uk_TCN_model, df, splitter, cleaner, TIMESTAMP, POWER, on_limit_w, off_limit_w, consecutive_points, seq_len, time_horizon, TARGET_COLUMN, error, temp_boundary, None)\n",
    "\n",
    "uk_TCN_results = {**uk_TCN_test_results, **uk_TCN_flex_results}\n",
    "uk_TCN_results[\"title\"] = \"TCN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51075bcb",
   "metadata": {},
   "source": [
    "# Uk Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pillar_diagrams([\"on mafe\", \"on maufe\", \"on maofe\", \"on epfr\", \"off mafe\", \"off maufe\", \"off maofe\", \"off epfr\"], \n",
    "                     [[uk_lstm_results, uk_GRU_results, uk_TCN_results]],\n",
    "                     [\"UK dataset\"],\n",
    "                     y_max=1.6)\n",
    "\n",
    "plot_pillar_diagrams([NRMSE.get_key(), NMAXE.get_key(), NMAE.get_key()], \n",
    "                     [[uk_lstm_results, uk_GRU_results, uk_TCN_results]],\n",
    "                     [\"UK dataset\"],\n",
    "                     y_max=1)\n",
    "\n",
    "lstm_predictions = uk_lstm_model.get_predictions()\n",
    "gru_predictions = uk_GRU_model.get_predictions()\n",
    "tcn_predictions = uk_TCN_model.get_predictions()\n",
    "\n",
    "plot_models(\n",
    "    [lstm_predictions, gru_predictions, tcn_predictions],\n",
    "    time_horizon,\n",
    "    uk_lstm_model.get_timestamps(),\n",
    "    uk_lstm_model.get_actuals(),\n",
    "    titels=[\"LSTM\", \"GRU\", \"TCN\"])\n",
    "\n",
    "plot_boxplots([lstm_predictions, gru_predictions, tcn_predictions, uk_lstm_model.get_actuals()],\n",
    "              titles=[\"LSTM\", \"GRU\", \"TCN\", \"Actuals\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf59dd",
   "metadata": {},
   "source": [
    "# MAFE for NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2950f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_mafe(results):\n",
    "    return (results['on mafe'] + results['off mafe']) / 2\n",
    "\n",
    "uk_lstm_results['mafe'] = calculate_mean_mafe(uk_lstm_results)\n",
    "uk_GRU_results['mafe'] = calculate_mean_mafe(uk_GRU_results)\n",
    "uk_TCN_results['mafe'] = calculate_mean_mafe(uk_TCN_results)\n",
    "\n",
    "uk = {}\n",
    "uk[NRMSE.get_key()] = [uk_lstm_results[NRMSE.get_key()], uk_GRU_results[NRMSE.get_key()], uk_TCN_results[NRMSE.get_key()]]\n",
    "uk[\"mafe\"] = [uk_lstm_results[\"mafe\"], uk_GRU_results[\"mafe\"], uk_TCN_results[\"mafe\"]]\n",
    "\n",
    "nist_lstm_results['mafe'] = calculate_mean_mafe(nist_lstm_results)\n",
    "nist_GRU_results['mafe'] = calculate_mean_mafe(nist_GRU_results)\n",
    "nist_TCN_results['mafe'] = calculate_mean_mafe(nist_TCN_results)\n",
    "\n",
    "nist = {}\n",
    "nist[NRMSE.get_key()] = [nist_lstm_results[NRMSE.get_key()], nist_GRU_results[NRMSE.get_key()], nist_TCN_results[NRMSE.get_key()]]\n",
    "nist[\"mafe\"] = [nist_lstm_results[\"mafe\"], nist_GRU_results[\"mafe\"], nist_TCN_results[\"mafe\"]]\n",
    "\n",
    "\n",
    "\n",
    "plot_metric_comparison([NRMSE.get_key(), \"mafe\"], [uk, nist], [\"UK dataset\", \"Nist dataset\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
