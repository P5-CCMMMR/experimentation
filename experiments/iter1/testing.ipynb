{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9249ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "from experiments.iter1.util.importer import *\n",
    "MODEL_PATH = 'model_saves/testing_model'\n",
    "nist_path = \"../../src/data_preprocess/dataset/UKDATA_cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1230e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:   1%|          | 1/100 [00:00<00:17,  5.52it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:   3%|▎         | 3/100 [00:00<00:08, 11.32it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:   6%|▌         | 6/100 [00:00<00:05, 15.70it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:   8%|▊         | 8/100 [00:00<00:05, 16.59it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  11%|█         | 11/100 [00:00<00:05, 15.07it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  13%|█▎        | 13/100 [00:00<00:05, 15.29it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:  16%|█▌        | 16/100 [00:01<00:04, 18.00it/s]\n",
      "\n",
      "\n",
      "Finding best initial lr:  19%|█▉        | 19/100 [00:01<00:03, 20.96it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  22%|██▏       | 22/100 [00:01<00:03, 20.86it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  25%|██▌       | 25/100 [00:01<00:03, 21.32it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  28%|██▊       | 28/100 [00:01<00:03, 21.36it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  31%|███       | 31/100 [00:01<00:03, 20.79it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  34%|███▍      | 34/100 [00:01<00:03, 21.17it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  37%|███▋      | 37/100 [00:01<00:02, 21.05it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  40%|████      | 40/100 [00:02<00:02, 20.80it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  43%|████▎     | 43/100 [00:02<00:02, 20.55it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  46%|████▌     | 46/100 [00:02<00:02, 20.33it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  49%|████▉     | 49/100 [00:02<00:02, 20.44it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  52%|█████▏    | 52/100 [00:02<00:02, 20.16it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  55%|█████▌    | 55/100 [00:02<00:02, 20.64it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  58%|█████▊    | 58/100 [00:03<00:02, 20.50it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  61%|██████    | 61/100 [00:03<00:01, 20.89it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  64%|██████▍   | 64/100 [00:03<00:01, 20.95it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  67%|██████▋   | 67/100 [00:03<00:01, 20.94it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  70%|███████   | 70/100 [00:03<00:01, 19.37it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  72%|███████▏  | 72/100 [00:09<00:18,  1.55it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  74%|███████▍  | 74/100 [00:09<00:13,  1.98it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  76%|███████▌  | 76/100 [00:09<00:09,  2.48it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  79%|███████▉  | 79/100 [00:09<00:05,  3.60it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  81%|████████  | 81/100 [00:09<00:04,  4.39it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  84%|████████▍ | 84/100 [00:09<00:02,  6.07it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  87%|████████▋ | 87/100 [00:09<00:01,  8.09it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  89%|████████▉ | 89/100 [00:10<00:01,  9.13it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  91%|█████████ | 91/100 [00:10<00:00, 10.37it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Finding best initial lr:  94%|█████████▍| 94/100 [00:10<00:00, 12.77it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Finding best initial lr:  96%|█████████▌| 96/100 [00:10<00:00, 13.63it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:10<00:00,  9.50it/s]\n",
      "Learning rate set to 4.786300923226383e-07\n",
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/iter1/.lr_find_a88a80c1-bfd6-4028-9d0d-24c351fc82bd.ckpt\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:10<00:00,  9.48it/s]Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/iter1/.lr_find_a88a80c1-bfd6-4028-9d0d-24c351fc82bd.ckpt\n",
      "\n",
      "Learning rate set to 4.786300923226383e-07\n",
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/iter1/.lr_find_2bfd3277-a6ff-44a6-871d-5dfea7e75054.ckpt\n",
      "\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:10<00:00, 13.75it/s]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:10<00:00,  9.44it/s]\n",
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/iter1/.lr_find_2bfd3277-a6ff-44a6-871d-5dfea7e75054.ckpt\n",
      "Learning rate set to 4.786300923226383e-07\n",
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/iter1/.lr_find_57e30073-5f7a-4ce1-bcd8-08fdcd633b31.ckpt\n",
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/iter1/.lr_find_57e30073-5f7a-4ce1-bcd8-08fdcd633b31.ckpt\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:10<00:00,  9.28it/s]\n",
      "Learning rate set to 4.786300923226383e-07\n",
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/iter1/.lr_find_e24b6969-5f13-4419-a9e4-3abc9d3a28e6.ckpt\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:10<00:00, 11.30it/s]Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/iter1/.lr_find_e24b6969-5f13-4419-a9e4-3abc9d3a28e6.ckpt\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:10<00:00,  9.18it/s]\n",
      "Learning rate set to 4.786300923226383e-07\n",
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/iter1/.lr_find_d27d1439-4b0b-488c-a800-738da7f6522d.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/iter1/.lr_find_d27d1439-4b0b-488c-a800-738da7f6522d.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 16 succeeded, trying batch size 32\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 16 succeeded, trying batch size 32\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 32 succeeded, trying batch size 64\n",
      "Batch size 16 succeeded, trying batch size 32\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 16 succeeded, trying batch size 32\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 16 succeeded, trying batch size 32\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 32 succeeded, trying batch size 64\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 32 succeeded, trying batch size 64\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 64 succeeded, trying batch size 128\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 32 succeeded, trying batch size 64\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 32 succeeded, trying batch size 64\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 64 succeeded, trying batch size 128\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 64 succeeded, trying batch size 128\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 128 succeeded, trying batch size 256\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 128 succeeded, trying batch size 256\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 64 succeeded, trying batch size 128\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 64 succeeded, trying batch size 128\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 128 succeeded, trying batch size 256\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 256 succeeded, trying batch size 512\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 256 succeeded, trying batch size 512\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 128 succeeded, trying batch size 256\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 128 succeeded, trying batch size 256\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 256 succeeded, trying batch size 512\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 512 succeeded, trying batch size 1024\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 256 succeeded, trying batch size 512\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 512 succeeded, trying batch size 1024\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 256 succeeded, trying batch size 512\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 512 succeeded, trying batch size 1024\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 1024 succeeded, trying batch size 2048\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 512 succeeded, trying batch size 1024\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 1024 succeeded, trying batch size 2048\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 512 succeeded, trying batch size 1024\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2048 succeeded, trying batch size 4096\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 1024 succeeded, trying batch size 2048\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 1024 succeeded, trying batch size 2048\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 1024 succeeded, trying batch size 2048\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2048 succeeded, trying batch size 4096\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4096 succeeded, trying batch size 8192\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2048 succeeded, trying batch size 4096\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2048 succeeded, trying batch size 4096\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2048 succeeded, trying batch size 4096\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4096 succeeded, trying batch size 8192\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8192 succeeded, trying batch size 16384\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4096 succeeded, trying batch size 8192\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4096 succeeded, trying batch size 8192\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4096 succeeded, trying batch size 8192\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8192 succeeded, trying batch size 16384\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "The batch size 16384 is greater or equal than the length of your dataset.\n",
      "Finished batch size finder, will continue with full run using batch size 16384\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8192 succeeded, trying batch size 16384\n",
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/iter1/.scale_batch_size_65f4cf1e-54d9-49f6-83b4-60c43e5e1bcc.ckpt\n",
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/iter1/.scale_batch_size_65f4cf1e-54d9-49f6-83b4-60c43e5e1bcc.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8192 succeeded, trying batch size 16384\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "The batch size 16384 is greater or equal than the length of your dataset.\n",
      "Batch size 8192 succeeded, trying batch size 16384\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | LSTM | 13.4 K | train\n",
      "---------------------------------------\n",
      "13.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.4 K    Total params\n",
      "0.054     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Finished batch size finder, will continue with full run using batch size 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/iter1/.scale_batch_size_190502b5-6c21-4b97-93fd-bba300816405.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/iter1/.scale_batch_size_190502b5-6c21-4b97-93fd-bba300816405.ckpt\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "The batch size 16384 is greater or equal than the length of your dataset.\n",
      "Finished batch size finder, will continue with full run using batch size 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▏         | 1/71 [00:00<00:18,  3.81it/s, v_num=294, train_loss_step=3.690]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/iter1/.scale_batch_size_98838daa-608a-424b-bf9d-f5f98e86241b.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 2/71 [00:00<00:16,  4.16it/s, v_num=294, train_loss_step=2.180]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "The batch size 16384 is greater or equal than the length of your dataset.\n",
      "Finished batch size finder, will continue with full run using batch size 16384\n",
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/iter1/.scale_batch_size_98838daa-608a-424b-bf9d-f5f98e86241b.ckpt\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "The batch size 16384 is greater or equal than the length of your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▍         | 3/71 [00:00<00:17,  3.95it/s, v_num=294, train_loss_step=2.180]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished batch size finder, will continue with full run using batch size 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|▋         | 5/71 [00:01<00:15,  4.34it/s, v_num=294, train_loss_step=0.668]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | LSTM | 13.4 K | train\n",
      "---------------------------------------\n",
      "13.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.4 K    Total params\n",
      "0.054     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|▋         | 5/71 [00:01<00:17,  3.85it/s, v_num=294, train_loss_step=0.668]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/iter1/.scale_batch_size_5c582091-dd95-4b07-b27c-295bea44bf30.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/iter1/.scale_batch_size_5c582091-dd95-4b07-b27c-295bea44bf30.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | LSTM | 13.4 K | train\n",
      "---------------------------------------\n",
      "13.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.4 K    Total params\n",
      "0.054     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  21%|██        | 15/71 [00:00<00:00, 68.06it/s, v_num=294, val_loss=1.200]       \n",
      "Epoch 0:  56%|█████▋    | 40/71 [00:01<00:01, 30.73it/s, v_num=294, train_loss_step=0.239]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  85%|████████▍ | 60/71 [00:01<00:00, 45.29it/s, v_num=294, train_loss_step=0.333]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | LSTM | 13.4 K | train\n",
      "---------------------------------------\n",
      "13.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.4 K    Total params\n",
      "0.054     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  85%|████████▍ | 60/71 [00:01<00:00, 44.53it/s, v_num=294, train_loss_step=0.333]\n",
      "Epoch 0:  82%|████████▏ | 58/71 [00:01<00:00, 46.02it/s, v_num=294, train_loss_step=1.310]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/iter1/.scale_batch_size_8e8088db-3286-4536-bcc6-432813798362.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  92%|█████████▏| 65/71 [00:01<00:00, 49.81it/s, v_num=294, train_loss_step=0.710]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/iter1/.scale_batch_size_8e8088db-3286-4536-bcc6-432813798362.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  18%|█▊        | 13/71 [00:01<00:06,  8.36it/s, v_num=294, train_loss_step=0.800]                                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  20%|█▉        | 14/71 [00:01<00:06,  8.69it/s, v_num=294, train_loss_step=0.466]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | LSTM | 13.4 K | train\n",
      "---------------------------------------\n",
      "13.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.4 K    Total params\n",
      "0.054     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  87%|████████▋ | 62/71 [00:01<00:00, 37.83it/s, v_num=294, train_loss_step=0.154]\n",
      "Epoch 29:  45%|████▌     | 32/71 [00:01<00:01, 26.23it/s, v_num=294, train_loss_step=0.115, val_loss=0.148, train_loss_epoch=0.169]   \n"
     ]
    }
   ],
   "source": [
    "assert time_horizon > 0, \"Time horizon must be a positive integer\"\n",
    "    \n",
    "df = pd.read_csv(nist_path)\n",
    "\n",
    "cleaner = TempCleaner(clean_pow_low, clean_in_low, clean_in_high, clean_out_low, clean_out_high, clean_delta_temp)\n",
    "splitter = StdSplitter(train_days, val_days, test_days)\n",
    "    \n",
    "model = LSTM(hidden_size, num_layers, input_size, time_horizon, dropout)\n",
    "trainer = TrainerWrapper(L.Trainer, \n",
    "                         max_epochs=num_epochs, \n",
    "                         callbacks=[ConditionalEarlyStopping(threshold=early_stopping_threshold)], \n",
    "                         gradient_clip_val=gradient_clipping)\n",
    "optimizer = OptimizerWrapper(optim.Adam, model, lr=learning_rate)\n",
    "\n",
    "model = DeterministicPipeline.Builder() \\\n",
    "        .add_data(df) \\\n",
    "        .set_cleaner(cleaner) \\\n",
    "        .set_normalizer_class(MinMaxNormalizer) \\\n",
    "        .set_splitter(splitter) \\\n",
    "        .set_sequencer_class(TimeSequencer) \\\n",
    "        .set_target_column(TARGET_COLUMN) \\\n",
    "        .set_model(model) \\\n",
    "        .set_optimizer(optimizer) \\\n",
    "        .set_batch_size(batch_size) \\\n",
    "        .set_seq_len(seq_len) \\\n",
    "        .set_worker_num(NUM_WORKERS) \\\n",
    "        .set_error(NRMSE) \\\n",
    "        .set_trainer(trainer) \\\n",
    "        .set_tuner_class(StdTunerWrapper) \\\n",
    "        .build()\n",
    "\n",
    "model = EnsemblePipeline.Builder() \\\n",
    "        .set_pipeline(model) \\\n",
    "        .set_num_ensembles(num_ensembles) \\\n",
    "        .set_horizon_len(time_horizon) \\\n",
    "        .build()\n",
    "\n",
    "model.fit()\n",
    "torch.save(model.state_dict(), MODEL_PATH + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e4cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:00<?, ?it/s]\n",
      "Testing:   0%|          | 0/9 [00:00<?, ?it/s]\n",
      "Testing DataLoader 0: |          | 0/? [00:00<?, ?it/s]\n",
      "Testing: |          | 0/? [00:00<?, ?it/s]\n",
      "Testing: |          | 9/? [00:00<00:00, 18.96it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   NRMSE Loss: _epoch        0.870110034942627\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   NRMSE Loss: _epoch       1.0230076313018799\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   NRMSE Loss: _epoch       0.9739409685134888\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   NRMSE Loss: _epoch       0.8404776453971863\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   NRMSE Loss: _epoch       0.8168626427650452\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "predictions length: 4440 | actuals len: 4440\n",
      "mean_arr length: 1110 | actuals len: 1110\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m#.add_test_error(NMCRPS) \\\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m#.add_test_error(NMLSCV) \\\u001b[39;00m\n\u001b[1;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(MODEL_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m---> 49\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m evaluate_model(model, df, splitter, cleaner, TIMESTAMP, POWER, on_limit_w, off_limit_w, consecutive_points, seq_len, time_horizon, TARGET_COLUMN, error, temp_boundary, \u001b[38;5;241m0.95\u001b[39m)\n",
      "File \u001b[0;32m~/P5/experimentation/src/pipelines/ensemble_pipeline.py:79\u001b[0m, in \u001b[0;36mEnsemblePipeline.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/P5/experimentation/src/pipelines/probabilistic_pipeline.py:60\u001b[0m, in \u001b[0;36m_log_test_errors\u001b[0;34m(self, mean_prediction, std_prediction, y)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/P5/experimentation/src/pipelines/metrics/rmse.py:32\u001b[0m, in \u001b[0;36mNRMSE.calc\u001b[0;34m(y_hat, y)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39mmin():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(y)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRMSE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (y\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mmin())\n",
      "File \u001b[0;32m~/P5/experimentation/src/pipelines/metrics/rmse.py:18\u001b[0m, in \u001b[0;36mRMSE.calc\u001b[0;34m(y_hat, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Add small term to avoid divison by zero\u001b[39;00m\n\u001b[1;32m     16\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-16\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m eps)\n",
      "File \u001b[0;32m~/P5/experimentation/.venv/lib/python3.10/site-packages/torch/nn/functional.py:3373\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   3370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3371\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[1;32m   3372\u001b[0m     )\n\u001b[0;32m-> 3373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3374\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3375\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3376\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3378\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3379\u001b[0m     )\n\u001b[1;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from experiments.iter1.util.evaluate import evaluate_model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.util.power_splitter import PowerSplitter\n",
    "from src.util.plotly import plot_results\n",
    "from src.pipelines.normalizers.min_max_normalizer import MinMaxNormalizer\n",
    "from src.util.evaluator import Evaluator\n",
    "\n",
    "df = pd.read_csv(nist_path)\n",
    "\n",
    "cleaner = TempCleaner(clean_pow_low, clean_in_low, clean_in_high, clean_out_low, clean_out_high, clean_delta_temp)\n",
    "splitter = StdSplitter(train_days, val_days, test_days)\n",
    "    \n",
    "model = LSTM(hidden_size, num_layers, input_size, time_horizon, dropout)\n",
    "trainer = TrainerWrapper(L.Trainer, \n",
    "                         max_epochs=num_epochs, \n",
    "                         callbacks=[ConditionalEarlyStopping(threshold=early_stopping_threshold)], \n",
    "                         gradient_clip_val=gradient_clipping)\n",
    "optimizer = OptimizerWrapper(optim.Adam, model, lr=learning_rate)\n",
    "\n",
    "model = DeterministicPipeline.Builder() \\\n",
    "        .add_data(df) \\\n",
    "        .set_cleaner(cleaner) \\\n",
    "        .set_normalizer_class(MinMaxNormalizer) \\\n",
    "        .set_splitter(splitter) \\\n",
    "        .set_sequencer_class(TimeSequencer) \\\n",
    "        .set_target_column(TARGET_COLUMN) \\\n",
    "        .set_model(model) \\\n",
    "        .set_optimizer(optimizer) \\\n",
    "        .set_batch_size(batch_size) \\\n",
    "        .set_seq_len(seq_len) \\\n",
    "        .set_worker_num(NUM_WORKERS) \\\n",
    "        .set_error(NRMSE) \\\n",
    "        .set_trainer(trainer) \\\n",
    "        .set_tuner_class(StdTunerWrapper) \\\n",
    "        .build()\n",
    "\n",
    "model = EnsemblePipeline.Builder() \\\n",
    "        .set_pipeline(model) \\\n",
    "        .set_num_ensembles(num_ensembles) \\\n",
    "        .set_horizon_len(time_horizon) \\\n",
    "        .set_error(NRMSE) \\\n",
    "        .build()\n",
    "\n",
    "        #.add_test_error(NMCRPS) \\\n",
    "        #.add_test_error(NMLSCV) \\\n",
    "model.load_state_dict(torch.load(MODEL_PATH + \".pth\", weights_only=True))\n",
    "model.test()\n",
    "\n",
    "\n",
    "evaluate_model(model, df, splitter, cleaner, TIMESTAMP, POWER, on_limit_w, off_limit_w, consecutive_points, seq_len, time_horizon, TARGET_COLUMN, error, temp_boundary, 0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
