{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1230e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:   1%|          | 1/100 [00:00<00:19,  5.18it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:   4%|▍         | 4/100 [00:00<00:07, 13.69it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:   6%|▌         | 6/100 [00:00<00:06, 15.33it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:   8%|▊         | 8/100 [00:00<00:05, 16.20it/s]\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  10%|█         | 10/100 [00:00<00:05, 17.00it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  12%|█▏        | 12/100 [00:00<00:06, 13.87it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  15%|█▌        | 15/100 [00:00<00:05, 16.52it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  17%|█▋        | 17/100 [00:01<00:04, 16.98it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:  20%|██        | 20/100 [00:01<00:04, 18.87it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  22%|██▏       | 22/100 [00:01<00:04, 18.84it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  25%|██▌       | 25/100 [00:01<00:03, 19.41it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  27%|██▋       | 27/100 [00:01<00:03, 19.52it/s]\n",
      "\u001b[A\n",
      "\n",
      "Finding best initial lr:  29%|██▉       | 29/100 [00:01<00:03, 18.91it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  31%|███       | 31/100 [00:01<00:03, 18.33it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  33%|███▎      | 33/100 [00:01<00:03, 18.42it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  35%|███▌      | 35/100 [00:02<00:03, 17.84it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  37%|███▋      | 37/100 [00:02<00:03, 16.69it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Finding best initial lr:  39%|███▉      | 39/100 [00:02<00:03, 16.85it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Finding best initial lr:  41%|████      | 41/100 [00:02<00:03, 17.03it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Finding best initial lr:  43%|████▎     | 43/100 [00:02<00:03, 16.96it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:  45%|████▌     | 45/100 [00:02<00:03, 17.36it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  48%|████▊     | 48/100 [00:02<00:02, 18.52it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  51%|█████     | 51/100 [00:02<00:02, 19.00it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  53%|█████▎    | 53/100 [00:03<00:02, 19.07it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:  55%|█████▌    | 55/100 [00:03<00:02, 18.97it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:  57%|█████▋    | 57/100 [00:03<00:02, 18.65it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:  59%|█████▉    | 59/100 [00:03<00:02, 18.39it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:  61%|██████    | 61/100 [00:03<00:02, 18.49it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:  63%|██████▎   | 63/100 [00:03<00:01, 18.54it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:  65%|██████▌   | 65/100 [00:03<00:01, 18.37it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  67%|██████▋   | 67/100 [00:03<00:01, 18.81it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  69%|██████▉   | 69/100 [00:03<00:01, 18.18it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  71%|███████   | 71/100 [00:04<00:01, 18.05it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  73%|███████▎  | 73/100 [00:04<00:01, 18.01it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  75%|███████▌  | 75/100 [00:04<00:01, 18.07it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  77%|███████▋  | 77/100 [00:04<00:01, 17.67it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  79%|███████▉  | 79/100 [00:04<00:01, 16.36it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  81%|████████  | 81/100 [00:04<00:01, 16.37it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  83%|████████▎ | 83/100 [00:04<00:01, 16.40it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  85%|████████▌ | 85/100 [00:04<00:00, 16.88it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  87%|████████▋ | 87/100 [00:04<00:00, 17.26it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Finding best initial lr:  89%|████████▉ | 89/100 [00:05<00:00, 17.64it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Finding best initial lr:  91%|█████████ | 91/100 [00:05<00:00, 17.98it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Finding best initial lr:  93%|█████████▎| 93/100 [00:05<00:00, 18.25it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:05<00:00, 17.97it/s]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
      "Learning rate set to 1.5848931924611133e-07\n",
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/.lr_find_64a554ef-67aa-48d5-a9f2-0a4b8a235c7a.ckpt\n",
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/.lr_find_64a554ef-67aa-48d5-a9f2-0a4b8a235c7a.ckpt\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:05<00:00, 18.01it/s]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:05<00:00, 18.13it/s]\n",
      "Learning rate set to 1.5848931924611133e-07\n",
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/.lr_find_0ba6b16a-32aa-4404-b55f-137bb7ccba5c.ckpt\n",
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/.lr_find_0ba6b16a-32aa-4404-b55f-137bb7ccba5c.ckpt\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:05<00:00, 17.88it/s]\n",
      "\n",
      "\n",
      "Learning rate set to 1.5848931924611133e-07\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:05<00:00, 16.79it/s]n/experiments/.lr_find_5a277bc5-7738-4733-9094-d792f4629df2.ckpt\u001b[ARestored all states from the checkpoint at /home/vind/P5/experimentation/experiments/.lr_find_5a277bc5-7738-4733-9094-d792f4629df2.ckpt\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:05<00:00, 17.51it/s]\n",
      "Learning rate set to 1.5848931924611133e-07\n",
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/.lr_find_f9ab3560-b0b4-433d-a4c8-69223ec402b5.ckpt\n",
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/.lr_find_f9ab3560-b0b4-433d-a4c8-69223ec402b5.ckpt\n",
      "\n",
      "\u001b[A`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:05<00:00, 17.29it/s]\n",
      "Learning rate set to 1.5848931924611133e-07\n",
      "Restoring states from the checkpoint path at /home/vind/P5/experimentation/experiments/.lr_find_beea3e3f-462c-429b-b7e0-edd8741ba93b.ckpt\n",
      "Restored all states from the checkpoint at /home/vind/P5/experimentation/experiments/.lr_find_beea3e3f-462c-429b-b7e0-edd8741ba93b.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4 succeeded, trying batch size 8\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming your project directory is one level up from the Jupyter notebook\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from src.pipelines.trainers.trainerWrapper import TrainerWrapper\n",
    "from src.util.conditional_early_stopping import ConditionalEarlyStopping\n",
    "from src.util.flex_error import get_mafe, get_prob_mafe\n",
    "from src.util.plot import plot_results\n",
    "from src.util.power_splitter import PowerSplitter\n",
    "from src.util.error import NRMSE\n",
    "\n",
    "from src.pipelines.cleaners.temp_cleaner import TempCleaner\n",
    "from src.pipelines.models.lstm import LSTM\n",
    "from src.pipelines.normalizers.min_max_normalizer import MinMaxNormalizer\n",
    "from src.pipelines.sequencers.time_sequencer import TimeSequencer\n",
    "from src.pipelines.splitters.std_splitter import StdSplitter\n",
    "from src.pipelines.tuners.std_tuner_wrapper import StdTunerWrapper\n",
    "from src.pipelines.optimizers.optimizer import OptimizerWrapper\n",
    "\n",
    "from src.pipelines.deterministic_pipeline import DeterministicPipeline\n",
    "from src.pipelines.monte_carlo_pipeline import MonteCarloPipeline\n",
    "from src.pipelines.ensemble_pipeline import EnsemblePipeline\n",
    "from src.pipelines.probabilistic_pipeline import ProbabilisticPipeline\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "MODEL_PATH = 'model.pth'\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "TARGET_COLUMN = 2\n",
    "TIMESTAMP = \"Timestamp\"\n",
    "POWER     = \"PowerConsumption\"\n",
    "\n",
    "# Hyper parameters\n",
    "# Model\n",
    "input_size = 4\n",
    "time_horizon = 4\n",
    "hidden_size = 32\n",
    "num_epochs = 1000\n",
    "seq_len = 96\n",
    "num_layers = 2\n",
    " \n",
    "# MC ONLY\n",
    "inference_samples = 50\n",
    "\n",
    "# Training\n",
    "dropout = 0.50\n",
    "gradient_clipping = 0\n",
    "early_stopping_threshold = 0.15\n",
    "\n",
    "num_ensembles = 5\n",
    "\n",
    "# Flexibility\n",
    "flex_confidence = 0.90\n",
    "temp_boundary = 0.1\n",
    "error = 0\n",
    "\n",
    "# Controlled by tuner\n",
    "batch_size = 128\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Data Split\n",
    "train_days = 16\n",
    "val_days = 2\n",
    "test_days = 2\n",
    "\n",
    "# ON / OFF Power Limits\n",
    "off_limit_w = 100\n",
    "on_limit_w = 1500\n",
    "\n",
    "consecutive_points = 3\n",
    "\n",
    "nist_path = \"../src/data_preprocess/dataset/NIST_cleaned.csv\"\n",
    "ukdata_path = \"../src/data_preprocess/ukdata/data_root/UKDATA_cleaned.csv\"\n",
    "\n",
    "clean_in_low = 10\n",
    "clean_in_high = 30\n",
    "clean_out_low = -50\n",
    "clean_out_high = 50\n",
    "clean_pow_low = 0\n",
    "clean_delta_temp = 15\n",
    "\n",
    "assert time_horizon > 0, \"Time horizon must be a positive integer\"\n",
    "    \n",
    "df = pd.read_csv(nist_path)\n",
    "\n",
    "cleaner = TempCleaner(clean_pow_low, clean_in_low, clean_in_high, clean_out_low, clean_out_high, clean_delta_temp)\n",
    "splitter = StdSplitter(train_days, val_days, test_days)\n",
    "    \n",
    "model = LSTM(hidden_size, num_layers, input_size, time_horizon, dropout)\n",
    "trainer = TrainerWrapper(L.Trainer, \n",
    "                         max_epochs=num_epochs, \n",
    "                         callbacks=[ConditionalEarlyStopping(threshold=early_stopping_threshold)], \n",
    "                         gradient_clip_val=gradient_clipping)\n",
    "optimizer = OptimizerWrapper(optim.Adam, model, lr=learning_rate)\n",
    "\n",
    "model = DeterministicPipeline.Builder() \\\n",
    "    .add_data(df) \\\n",
    "    .set_cleaner(cleaner) \\\n",
    "    .set_normalizer_class(MinMaxNormalizer) \\\n",
    "    .set_splitter(splitter) \\\n",
    "    .set_sequencer_class(TimeSequencer) \\\n",
    "    .set_target_column(TARGET_COLUMN) \\\n",
    "    .set_model(model) \\\n",
    "    .set_optimizer(optimizer) \\\n",
    "    .set_batch_size(batch_size) \\\n",
    "    .set_seq_len(seq_len) \\\n",
    "    .set_worker_num(NUM_WORKERS) \\\n",
    "    .set_error(NRMSE) \\\n",
    "    .set_trainer(trainer) \\\n",
    "    .set_tuner_class(StdTunerWrapper) \\\n",
    "    .build()\n",
    "\n",
    "model = EnsemblePipeline.Builder() \\\n",
    "        .set_pipeline(model) \\\n",
    "        .set_num_ensembles(num_ensembles) \\\n",
    "        .build()\n",
    "\n",
    "model.fit()\n",
    "model.test()\n",
    "\n",
    "plot_results(model.get_predictions(), model.get_actuals(), model.get_timestamps())\n",
    "\n",
    "model.eval()\n",
    "\n",
    "ps = PowerSplitter(splitter.get_test(cleaner.clean(df)), TIMESTAMP, POWER)\n",
    "\n",
    "on_df = ps.get_mt_power(on_limit_w, consecutive_points)\n",
    "off_df = ps.get_lt_power(off_limit_w, consecutive_points)\n",
    "\n",
    "on_data = np.array(on_df)\n",
    "off_data = np.array(off_df)\n",
    "\n",
    "print(\"Calculating mafe...\")\n",
    "if (isinstance(model, ProbabilisticPipeline)):\n",
    "    print(\"PROB MAFE ON:\", get_prob_mafe(on_data, model, seq_len, error, temp_boundary, time_horizon, TARGET_COLUMN, flex_confidence))\n",
    "    print(\"PROB MAFE OFF:\", get_prob_mafe(off_data, model, seq_len, error, temp_boundary, time_horizon, TARGET_COLUMN, flex_confidence))\n",
    "else:\n",
    "    print(\"MAFE ON:\", get_mafe(on_data, model, seq_len, error, temp_boundary, time_horizon, TARGET_COLUMN))\n",
    "    print(\"MAFE OFF:\", get_mafe(off_data, model, seq_len, error, temp_boundary, time_horizon, TARGET_COLUMN))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
